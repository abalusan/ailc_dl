{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Objectives </h3> \n",
    "Usage of loss functions in Pytorch. <br>\n",
    "Understanding python implementation for loss functions. <br>\n",
    "Check output equivalance b/w pytorch implementation and the python custom implementation. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7632e+00, 2.1812e-01, 1.8035e+00, 1.5246e-01, 3.7519e+00],\n",
      "        [4.3931e-01, 3.3247e+00, 4.6090e-03, 7.2474e+00, 3.2145e+00],\n",
      "        [1.8596e+00, 1.3716e+01, 1.6387e-01, 3.1076e+00, 7.0444e-01],\n",
      "        [5.5768e-02, 2.4461e-02, 9.8400e+00, 5.9140e-01, 9.2709e-04]])\n",
      "tensor(2.6492)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.6492)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MSE\n",
    "prediction = t.randn(4,5) # 4 rows 5 cols or batch size 4 and feature size 5.\n",
    "label = t.randn(4,5)\n",
    "\n",
    "# Pytorch imprementation\n",
    "mse = nn.MSELoss(reduction = 'none')\n",
    "loss = mse(prediction, label)\n",
    "print(loss)\n",
    "\n",
    "mse = nn.MSELoss(reduction = 'mean') # similary we can use sum ( we take sum/mean of errors across all the outputs to get total error. This is what reduction means.)\n",
    "loss = mse(prediction, label)\n",
    "print(loss)\n",
    "\n",
    "# Python custom implementation\n",
    "((prediction-label)**2).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 1., 0.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1.]])\n",
      "tensor(0.8090)\n",
      "tensor(0.8090)\n",
      "[[2.2122765, 0.19659343, 1.6378449879259707, 0.38734385, 1.0426361162736406], [0.36980586800146714, 0.22323878, 0.9918027, 0.24783188634914483, 0.39702022842707235], [1.1681699, 0.8140066, 0.7684105544714361, 0.4328358710707889, 0.3078309959305057], [0.620236701232242, 0.6643943193804518, 1.5443932, 1.6447643, 0.5080149]]\n"
     ]
    }
   ],
   "source": [
    "#BCE binary cross entropy\n",
    "label = t.zeros(4,5).random_(0,2) #Upper bound is non inclusive so we need to give 0,2\n",
    "print(label)\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "#Torch implementation ( doesn't use lists)\n",
    "bce = nn.BCELoss(reduction='mean')\n",
    "print(bce(sigmoid(prediction), label))\n",
    "\n",
    "bces = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "#w/ bces we dont have to use sigmoid as it is part of the torch implementation\n",
    "print(bces(prediction,label))\n",
    "\n",
    "#Custom python implementation\n",
    "import numpy as np\n",
    "pred_np = prediction.numpy()\n",
    "label_np = label.numpy()\n",
    "#Pass through sigmoid to convert it into a range of [0,1]\n",
    "def sigmoid(x: list[float]) -> list[float]:\n",
    "    return 1/(1+ np.exp(-x))\n",
    "\n",
    "pred_np = sigmoid(pred_np)\n",
    "loss_values = []\n",
    "\n",
    "for a in range(len(pred_np)):\n",
    "    batch_loss = []\n",
    "    for b in range(len(pred_np[0])):\n",
    "        batch_loss.append(-np.log(pred_np[a][b]) if label[a][b] == 1 else -np.log(1-pred_np[a][b]))\n",
    "    loss_values.append(batch_loss)\n",
    "print(loss_values) \n",
    "\n",
    "#Cross-entropy loss\n",
    "#In pytorch for Cross-entropy loss we dont have to provide a one-hot vector, instead, we can just give the a tensor that has the indices of the labels. \n",
    "# Ex: batch size 3, classification 2 classes: their indices 0,1\n",
    "# input will be (batch, classes) (3,2) and target would be 1d vector of size (batch,) (3,) w/ index of the correct class. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".course_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
