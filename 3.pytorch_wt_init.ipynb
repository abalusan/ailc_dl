{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3791, -0.1486, -0.2082,  0.4022,  0.1275],\n",
      "        [-0.2144,  0.0832,  0.1472, -0.3039, -0.2183],\n",
      "        [ 0.0668,  0.3618,  0.2502,  0.0326, -0.0055],\n",
      "        [-0.2480, -0.1907, -0.2823,  0.3393, -0.1616],\n",
      "        [-0.2716,  0.0959,  0.1024,  0.3893, -0.2500]])\n"
     ]
    }
   ],
   "source": [
    "#Layer weights that we want to initialize\n",
    "layer = nn.Linear(5,5)\n",
    "#Initialized w/ xavier techique by default\n",
    "#layer.weight\n",
    "print(layer.weight.data) #to just get the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.linear.Linear'>\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(layer))\n",
    "print(type(layer.weight))\n",
    "print(type(layer.weight.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.8226, 2.8127, 0.1642, 0.2529, 0.2711],\n",
       "        [0.7086, 2.5026, 0.5879, 0.4848, 0.1943],\n",
       "        [2.8112, 2.3834, 2.8811, 2.8797, 2.1099],\n",
       "        [0.8597, 1.6499, 0.0597, 0.2205, 0.0852],\n",
       "        [0.5828, 0.6215, 1.0596, 2.7675, 1.5029]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing weights based on uniform distribution\n",
    "nn.init.uniform_(layer.weight.data, a=0.0, b=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0361,  0.0695, -0.0010, -0.0139, -0.1606],\n",
       "        [-0.1200, -0.0493, -0.0207,  0.0778,  0.1833],\n",
       "        [-0.2520, -0.2851, -0.1612,  0.4226, -0.1166],\n",
       "        [-0.1278,  0.1401,  0.0753,  0.1644,  0.2777],\n",
       "        [-0.1082, -0.0495,  0.1220, -0.1277, -0.0331]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normal distribution\n",
    "nn.init.normal_(layer.weight.data, mean=0.0, std=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([8., 8., 8., 8., 8.], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constant value - Do not do this for weights as there will be no learning\n",
    "# Useful for initializing the biases. Most cases we initialize the bias to 0. Purpose of bias is to introduce some offset and non linearity due to that. We can achieve the same with activations.\n",
    "nn.init.constant_(layer.bias, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another way to initialize 0's to all the biases in the layer\n",
    "nn.init.zeros_(layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5207,  1.5322, -0.5979, -0.6783, -0.7515],\n",
       "        [ 0.2383,  0.1587,  0.1287, -1.8495, -0.6289],\n",
       "        [ 0.6154, -0.8387, -0.8420,  1.2033, -0.8966],\n",
       "        [-0.7215, -1.1085, -0.5624, -0.6188, -0.0341],\n",
       "        [-1.5191,  1.2468, -1.2917, -0.5344,  1.9517]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Xavier initialization explicitly. This is generated fro a uniform distribution. The range [-a,a] is calculated based on fan_in and fan_out of layers\n",
    "nn.init.xavier_uniform_(layer.weight, gain= 1.0)\n",
    "nn.init.xavier_normal_(layer.weight, gain=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".course_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
